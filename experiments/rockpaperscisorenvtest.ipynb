{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im [rlHelloWorld](experiments/rlHelloWorld.ipynb) wurde ein anderes reinforcement learning framework genutzt, welches glaube ich eng mit tensorflow zusammenarbeitet. Die Libraries unten sind aber die originalen von Tensorflow aus dem professionellen Cart Pole Tutorial von Tensorflow [Tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS_Game(py_environment.PyEnvironment):\n",
    "    \n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=[0], maximum=[2], name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = np.random.randint(0, 2)\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Rock = 0, Paper = 1, Scissors = 2\n",
    "    reward = 0\n",
    "    if action == 0:\n",
    "        if self._state == 1:\n",
    "            reward = -1\n",
    "        elif self._state == 2:\n",
    "            reward = 1\n",
    "    elif action == 1:\n",
    "        if self._state == 0:\n",
    "            reward = 1\n",
    "        elif self._state == 2:\n",
    "            reward = -1\n",
    "    elif action == 2:\n",
    "        if self._state == 0:\n",
    "            reward = -1\n",
    "        elif self._state == 1:\n",
    "            reward = 1\n",
    "    else:\n",
    "        raise ValueError('`action` should be 0 or 1.')\n",
    "    \n",
    "    self._episode_ended = True\n",
    "    return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    \n",
    "\n",
    "    ''' if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RPS_Game()\n",
    "env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with the environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)),\n",
       " BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array([0], dtype=int32), maximum=array([2], dtype=int32)))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec(), env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array([0], dtype=int32), maximum=array([2], dtype=int32)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartPoleEnv = suite_gym.load(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1),\n",
       " BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartPoleEnv.action_spec(), cartPoleEnv.observation_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50) # Die Anzahl der Dense Units in einem Layer\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent creation\n",
    "Dieser Agent wird nun über das Tensorflow eigene RL System erstellt und nicht über eine dritte Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy # main policy\n",
    "collect_policy = agent.collect_policy # secondary policy to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "                                                env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    \n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(env, random_policy, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer (Data Contaier) and Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmpr0qxs2f1.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:386] Loading latest checkpoint from /tmp/tmpr0qxs2f1\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 18657\n"
     ]
    }
   ],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=100000,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  'observation': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n",
       "  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}),\n",
       " ())"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        random_policy, use_tf_function=True),\n",
    "        [rb_observer],\n",
    "        max_steps=100\n",
    "    ).run(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(Trajectory(\n",
       "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 1), dtype=tf.int32, name=None),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None)))>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fe0c84c3310>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Training\n",
    "TODO: WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7, <tf_agents.policies.greedy_policy.GreedyPolicy at 0x7fe28c0e1940>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the agent's policy once before training.\n",
    "compute_avg_return(env, agent.policy, 10), agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(env, agent.policy, 100)\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment.\n",
    "time_step = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supports batched time steps with a single batch dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb#ch0000030vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb#ch0000030vscode-remote?line=1'>2</a>\u001b[0m     \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb#ch0000030vscode-remote?line=2'>3</a>\u001b[0m   \u001b[39m# Collect a few steps and save to the replay buffer.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb#ch0000030vscode-remote?line=3'>4</a>\u001b[0m   time_step, _ \u001b[39m=\u001b[39m collect_driver\u001b[39m.\u001b[39;49mrun(time_step)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb#ch0000030vscode-remote?line=5'>6</a>\u001b[0m   \u001b[39m# Sample a batch of data from the buffer and update the agent's network.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/GitHub/Nachzeichner-KI/experiments/rockpaperscisorenvtest.ipynb#ch0000030vscode-remote?line=6'>7</a>\u001b[0m   experience, unused_info \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py:110\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbatched \u001b[39mand\u001b[39;00m time_step\u001b[39m.\u001b[39mis_first() \u001b[39mand\u001b[39;00m num_episodes \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=107'>108</a>\u001b[0m   policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mget_initial_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbatch_size \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=109'>110</a>\u001b[0m action_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49maction(time_step, policy_state)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=110'>111</a>\u001b[0m next_time_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action_step\u001b[39m.\u001b[39maction)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=112'>113</a>\u001b[0m \u001b[39m# When using observer (for the purpose of training), only the previous\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=113'>114</a>\u001b[0m \u001b[39m# policy_state is useful. Therefore substitube it in the PolicyStep and\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/drivers/py_driver.py?line=114'>115</a>\u001b[0m \u001b[39m# consume it w/ the observer.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_policy.py:161\u001b[0m, in \u001b[0;36mPyPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_policy.py?line=158'>159</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action(time_step, policy_state, seed\u001b[39m=\u001b[39mseed)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_policy.py?line=159'>160</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_policy.py?line=160'>161</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action(time_step, policy_state)\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_tf_eager_policy.py:104\u001b[0m, in \u001b[0;36mPyTFEagerPolicyBase._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_tf_eager_policy.py?line=101'>102</a>\u001b[0m   policy_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy_action_fn(time_step, policy_state, seed\u001b[39m=\u001b[39mseed)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_tf_eager_policy.py?line=102'>103</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_tf_eager_policy.py?line=103'>104</a>\u001b[0m   policy_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_action_fn(time_step, policy_state)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_tf_eager_policy.py?line=104'>105</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_time_steps:\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/py_tf_eager_policy.py?line=105'>106</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m policy_step\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py:323\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py?line=320'>321</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py?line=321'>322</a>\u001b[0m   policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py?line=322'>323</a>\u001b[0m step \u001b[39m=\u001b[39m action_fn(time_step\u001b[39m=\u001b[39;49mtime_step, policy_state\u001b[39m=\u001b[39;49mpolicy_state, seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py?line=324'>325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip_action\u001b[39m(action, action_spec):\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py?line=325'>326</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(action_spec, tensor_spec\u001b[39m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=183'>184</a>\u001b[0m check_tf1_allowed()\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=184'>185</a>\u001b[0m \u001b[39mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=185'>186</a>\u001b[0m   \u001b[39m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=186'>187</a>\u001b[0m   \u001b[39m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=187'>188</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=188'>189</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/utils/common.py?line=189'>190</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m/mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py:130\u001b[0m, in \u001b[0;36mEpsilonGreedyPolicy._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=127'>128</a>\u001b[0m outer_ndims \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(outer_shape\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=128'>129</a>\u001b[0m \u001b[39mif\u001b[39;00m outer_ndims \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=129'>130</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=130'>131</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mOnly supports batched time steps with a single batch dimension\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=131'>132</a>\u001b[0m action \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m g, r: tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mwhere(cond, g, r),\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=132'>133</a>\u001b[0m                                greedy_action\u001b[39m.\u001b[39maction, random_action\u001b[39m.\u001b[39maction)\n\u001b[1;32m    <a href='file:///mnt/c/GitHub/Nachzeichner-KI/env/lib/python3.8/site-packages/tf_agents/policies/epsilon_greedy_policy.py?line=134'>135</a>\u001b[0m \u001b[39mif\u001b[39;00m greedy_action\u001b[39m.\u001b[39minfo:\n",
      "\u001b[0;31mValueError\u001b[0m: Only supports batched time steps with a single batch dimension"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    \n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % 200 == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % 200 == 0:\n",
    "    avg_return = compute_avg_return(env, agent.policy, 100)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "291d700669bd43c54f0c30a4fcf2f5d664ab06a34b2f6283f605dda78815dbcc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
